{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm version: 4.6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import  Booster, LGBMClassifier\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(f\"lightgbm version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "test_data = pd.read_csv('data/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuild the LGBM Claasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('lgbm',\n",
      "                 LGBMClassifier(max_depth=5, n_jobs=-1, random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "booster_model = Booster(\n",
    "    model_file='model.txt'\n",
    ")\n",
    "\n",
    "# restore model from pickle file\n",
    "with open('pipeline.pkl', 'rb') as f:\n",
    "    pipeline2 = pickle.load(f)\n",
    "\n",
    "print(pipeline2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [('scaler', StandardScaler()),\n",
       "  ('lgbm', LGBMClassifier(max_depth=5, n_jobs=-1, random_state=42))],\n",
       " 'memory': None,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaler': StandardScaler(),\n",
       " 'lgbm': LGBMClassifier(max_depth=5, n_jobs=-1, random_state=42)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Booster <lightgbm.basic.Booster object at 0xffff29f75bd0>\n",
      "_LGBMClassifier__is_multiclass False\n",
      ">>>>>>Error setting _LGBMClassifier__is_multiclass\n",
      "_base_doc \n",
      "    Build a gradient boosting model from the training set (X, y).\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    X : numpy array, pandas DataFrame, H2O DataTable's Frame (deprecated), scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]\n",
      "        Input feature matrix.\n",
      "    y : numpy array, pandas DataFrame, pandas Series, list of int or float of shape = [n_samples]\n",
      "        The target values (class labels in classification, real numbers in regression).\n",
      "    sample_weight : numpy array, pandas Series, list of int or float of shape = [n_samples] or None, optional (default=None)\n",
      "        Weights of training data. Weights should be non-negative.\n",
      "    init_score : numpy array, pandas DataFrame, pandas Series, list of int or float of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task) or shape = [n_samples, n_classes] (for multi-class task) or None, optional (default=None)\n",
      "        Init score of training data.\n",
      "    eval_set : list or None, optional (default=None)\n",
      "        A list of (X, y) tuple pairs to use as validation sets.\n",
      "    eval_names : list of str, or None, optional (default=None)\n",
      "        Names of eval_set.\n",
      "    eval_sample_weight : list of array (same types as ``sample_weight`` supports), or None, optional (default=None)\n",
      "        Weights of eval data. Weights should be non-negative.\n",
      "    eval_class_weight : list or None, optional (default=None)\n",
      "        Class weights of eval data.\n",
      "    eval_init_score : list of array (same types as ``init_score`` supports), or None, optional (default=None)\n",
      "        Init score of eval data.\n",
      "    eval_group : list of array (same types as ``group`` supports), or None, optional (default=None)\n",
      "        Group data of eval data.\n",
      "    eval_metric : str, callable, list or None, optional (default=None)\n",
      "        If str, it should be a built-in evaluation metric to use.\n",
      "        If callable, it should be a custom evaluation metric, see note below for more details.\n",
      "        If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both.\n",
      "        In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      "        Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      "    feature_name : list of str, or 'auto', optional (default='auto')\n",
      "        Feature names.\n",
      "        If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      "    categorical_feature : list of str or int, or 'auto', optional (default='auto')\n",
      "        Categorical features.\n",
      "        If list of int, interpreted as indices.\n",
      "        If list of str, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      "        If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      "        All values in categorical features will be cast to int32 and thus should be less than int32 max value (2147483647).\n",
      "        Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      "        All negative values in categorical features will be treated as missing values.\n",
      "        The output cannot be monotonically constrained with respect to a categorical feature.\n",
      "        Floating point numbers in categorical features will be rounded towards 0.\n",
      "    callbacks : list of callable, or None, optional (default=None)\n",
      "        List of callback functions that are applied at each iteration.\n",
      "        See Callbacks in Python API for more information.\n",
      "    init_model : str, pathlib.Path, Booster, LGBMModel or None, optional (default=None)\n",
      "        Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    self : LGBMClassifier\n",
      "        Returns self.\n",
      "    \n",
      "\n",
      "\n",
      "    Note\n",
      "    ----\n",
      "    Custom eval function expects a callable with following signatures:\n",
      "    ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      "    ``func(y_true, y_pred, weight, group)``\n",
      "    and returns (eval_name, eval_result, is_higher_better) or\n",
      "    list of (eval_name, eval_result, is_higher_better):\n",
      "\n",
      "        y_true : numpy 1-D array of shape = [n_samples]\n",
      "            The target values.\n",
      "        y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      "            The predicted values.\n",
      "            In case of custom ``objective``, predicted values are returned before any transformation,\n",
      "            e.g. they are raw margin instead of probability of positive class for binary task in this case.\n",
      "        weight : numpy 1-D array of shape = [n_samples]\n",
      "            The weight of samples. Weights should be non-negative.\n",
      "        group : numpy 1-D array\n",
      "            Group/query data.\n",
      "            Only used in the learning-to-rank task.\n",
      "            sum(group) = n_samples.\n",
      "            For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      "            where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      "        eval_name : str\n",
      "            The name of evaluation function (without whitespace).\n",
      "        eval_result : float\n",
      "            The eval result.\n",
      "        is_higher_better : bool\n",
      "            Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      "\n",
      "_best_iteration None\n",
      "_best_score defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('binary_logloss', np.float64(0.24494811239205372))])})\n",
      "_class_map {np.int64(0): np.int64(0), np.int64(1): np.int64(1)}\n",
      "_class_weight None\n",
      "_classes [0 1]\n",
      "_doc_link_module sklearn\n",
      "_doc_link_template https://scikit-learn.org/1.5/modules/generated/{estimator_module}.{estimator_name}.html\n",
      "_doc_link_url_param_generator None\n",
      "_estimator_type classifier\n",
      "_evals_result {'valid_0': OrderedDict([('binary_logloss', [np.float64(0.6224514724503258), np.float64(0.5640742468663877), np.float64(0.5161954200573156), np.float64(0.4752017549893619), np.float64(0.44221695759239493), np.float64(0.41339636783024153), np.float64(0.3881670127190147), np.float64(0.36719666129139017), np.float64(0.3484656461030392), np.float64(0.33257361806326075), np.float64(0.31854994749396026), np.float64(0.30711339797540127), np.float64(0.2960181811117945), np.float64(0.2869151657469304), np.float64(0.27884594715502886), np.float64(0.27200016232100405), np.float64(0.26566794532012844), np.float64(0.26067114850636286), np.float64(0.2557461382190177), np.float64(0.2525005950608412), np.float64(0.248571880351936), np.float64(0.24663213688614252), np.float64(0.24465641010449984), np.float64(0.24270189799289418), np.float64(0.24087191142326905), np.float64(0.23962434709618585), np.float64(0.23859996006723097), np.float64(0.2374428630886635), np.float64(0.23773537545468856), np.float64(0.2366048788629743), np.float64(0.23551194411506043), np.float64(0.23519218722558594), np.float64(0.2346584923604456), np.float64(0.2339425925567723), np.float64(0.23379339722927528), np.float64(0.23440774719174765), np.float64(0.2340625882724724), np.float64(0.23391065546383746), np.float64(0.23351092313976093), np.float64(0.23386991963804551), np.float64(0.2336520389782635), np.float64(0.23345291583876782), np.float64(0.23311198209446116), np.float64(0.23294347149751565), np.float64(0.23348980884509607), np.float64(0.2337082111750412), np.float64(0.2342842345078521), np.float64(0.23467851116756067), np.float64(0.23497667527342586), np.float64(0.23492097789654243), np.float64(0.23499496943484147), np.float64(0.23540424406620825), np.float64(0.23527511366981924), np.float64(0.23547501521419784), np.float64(0.23545334538932555), np.float64(0.23554072815736218), np.float64(0.23642345843716747), np.float64(0.23586134196772943), np.float64(0.23627033457119462), np.float64(0.23633695512296587), np.float64(0.237064474306381), np.float64(0.23684036433879985), np.float64(0.23648040629775735), np.float64(0.2363825113682473), np.float64(0.2361025633297508), np.float64(0.23588977970186195), np.float64(0.23595758820651644), np.float64(0.23678873765221733), np.float64(0.2368028910553893), np.float64(0.23677835709815823), np.float64(0.2378367389197298), np.float64(0.237754247099788), np.float64(0.23790264093645702), np.float64(0.238426617875546), np.float64(0.23905946380813323), np.float64(0.23958387863535616), np.float64(0.2399190962162774), np.float64(0.23988689267477592), np.float64(0.23999977524295296), np.float64(0.2407988797527143), np.float64(0.24092427215278656), np.float64(0.24087115950491889), np.float64(0.24091417767087261), np.float64(0.2412609720619352), np.float64(0.2416449187747535), np.float64(0.24214993893092024), np.float64(0.24261168665693233), np.float64(0.24245658854841623), np.float64(0.2427434454115014), np.float64(0.2424357463679045), np.float64(0.242745081551131), np.float64(0.24356656398354143), np.float64(0.24409857118495307), np.float64(0.24396436843135827), np.float64(0.24499191117697708), np.float64(0.24483849025464965), np.float64(0.2444738146901536), np.float64(0.24490111246923998), np.float64(0.24508315593210372), np.float64(0.24494811239205372)])])}\n",
      "_fobj None\n",
      "_le LabelEncoder()\n",
      "_n_classes 2\n",
      "_n_features 10\n",
      "_n_features_in 10\n",
      "_objective binary\n",
      "_other_params {}\n",
      "best_iteration_ None\n",
      ">>>>>>Error setting best_iteration_\n",
      "best_score_ defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('binary_logloss', np.float64(0.24494811239205372))])})\n",
      ">>>>>>Error setting best_score_\n",
      "booster_ <lightgbm.basic.Booster object at 0xffff29f75bd0>\n",
      ">>>>>>Error setting booster_\n",
      "boosting_type gbdt\n",
      "class_weight None\n",
      "classes_ [0 1]\n",
      ">>>>>>Error setting classes_\n",
      "colsample_bytree 1.0\n",
      "evals_result_ {'valid_0': OrderedDict([('binary_logloss', [np.float64(0.6224514724503258), np.float64(0.5640742468663877), np.float64(0.5161954200573156), np.float64(0.4752017549893619), np.float64(0.44221695759239493), np.float64(0.41339636783024153), np.float64(0.3881670127190147), np.float64(0.36719666129139017), np.float64(0.3484656461030392), np.float64(0.33257361806326075), np.float64(0.31854994749396026), np.float64(0.30711339797540127), np.float64(0.2960181811117945), np.float64(0.2869151657469304), np.float64(0.27884594715502886), np.float64(0.27200016232100405), np.float64(0.26566794532012844), np.float64(0.26067114850636286), np.float64(0.2557461382190177), np.float64(0.2525005950608412), np.float64(0.248571880351936), np.float64(0.24663213688614252), np.float64(0.24465641010449984), np.float64(0.24270189799289418), np.float64(0.24087191142326905), np.float64(0.23962434709618585), np.float64(0.23859996006723097), np.float64(0.2374428630886635), np.float64(0.23773537545468856), np.float64(0.2366048788629743), np.float64(0.23551194411506043), np.float64(0.23519218722558594), np.float64(0.2346584923604456), np.float64(0.2339425925567723), np.float64(0.23379339722927528), np.float64(0.23440774719174765), np.float64(0.2340625882724724), np.float64(0.23391065546383746), np.float64(0.23351092313976093), np.float64(0.23386991963804551), np.float64(0.2336520389782635), np.float64(0.23345291583876782), np.float64(0.23311198209446116), np.float64(0.23294347149751565), np.float64(0.23348980884509607), np.float64(0.2337082111750412), np.float64(0.2342842345078521), np.float64(0.23467851116756067), np.float64(0.23497667527342586), np.float64(0.23492097789654243), np.float64(0.23499496943484147), np.float64(0.23540424406620825), np.float64(0.23527511366981924), np.float64(0.23547501521419784), np.float64(0.23545334538932555), np.float64(0.23554072815736218), np.float64(0.23642345843716747), np.float64(0.23586134196772943), np.float64(0.23627033457119462), np.float64(0.23633695512296587), np.float64(0.237064474306381), np.float64(0.23684036433879985), np.float64(0.23648040629775735), np.float64(0.2363825113682473), np.float64(0.2361025633297508), np.float64(0.23588977970186195), np.float64(0.23595758820651644), np.float64(0.23678873765221733), np.float64(0.2368028910553893), np.float64(0.23677835709815823), np.float64(0.2378367389197298), np.float64(0.237754247099788), np.float64(0.23790264093645702), np.float64(0.238426617875546), np.float64(0.23905946380813323), np.float64(0.23958387863535616), np.float64(0.2399190962162774), np.float64(0.23988689267477592), np.float64(0.23999977524295296), np.float64(0.2407988797527143), np.float64(0.24092427215278656), np.float64(0.24087115950491889), np.float64(0.24091417767087261), np.float64(0.2412609720619352), np.float64(0.2416449187747535), np.float64(0.24214993893092024), np.float64(0.24261168665693233), np.float64(0.24245658854841623), np.float64(0.2427434454115014), np.float64(0.2424357463679045), np.float64(0.242745081551131), np.float64(0.24356656398354143), np.float64(0.24409857118495307), np.float64(0.24396436843135827), np.float64(0.24499191117697708), np.float64(0.24483849025464965), np.float64(0.2444738146901536), np.float64(0.24490111246923998), np.float64(0.24508315593210372), np.float64(0.24494811239205372)])])}\n",
      ">>>>>>Error setting evals_result_\n",
      "feature_importances_ [284 122 335 334 279 171 179 195 133 187]\n",
      ">>>>>>Error setting feature_importances_\n",
      "feature_name_ ['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5', 'Column_6', 'Column_7', 'Column_8', 'Column_9']\n",
      ">>>>>>Error setting feature_name_\n",
      "feature_names_in_ ['Column_0' 'Column_1' 'Column_2' 'Column_3' 'Column_4' 'Column_5'\n",
      " 'Column_6' 'Column_7' 'Column_8' 'Column_9']\n",
      ">>>>>>Error setting feature_names_in_\n",
      "fitted_ True\n",
      "importance_type split\n",
      "learning_rate 0.1\n",
      "max_depth 5\n",
      "min_child_samples 20\n",
      "min_child_weight 0.001\n",
      "min_split_gain 0.0\n",
      "n_classes_ 2\n",
      ">>>>>>Error setting n_classes_\n",
      "n_estimators 100\n",
      "n_estimators_ 100\n",
      ">>>>>>Error setting n_estimators_\n",
      "n_features_ 10\n",
      ">>>>>>Error setting n_features_\n",
      "n_features_in_ 10\n",
      "n_iter_ 100\n",
      ">>>>>>Error setting n_iter_\n",
      "n_jobs -1\n",
      "num_leaves 31\n",
      "objective None\n",
      "objective_ binary\n",
      ">>>>>>Error setting objective_\n",
      "random_state 42\n",
      "reg_alpha 0.0\n",
      "reg_lambda 0.0\n",
      "silent True\n",
      "subsample 1.0\n",
      "subsample_for_bin 200000\n",
      "subsample_freq 0\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('lgbm',\n",
      "                 LGBMClassifier(max_depth=5, n_jobs=-1, random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create stub classier\n",
    "model2 = LGBMClassifier()\n",
    "\n",
    "# populae the stub classifier with the attributes of the original model\n",
    "mdl = pipeline2.named_steps['lgbm']\n",
    "for p in dir(mdl):\n",
    "    if not p.startswith(\"__\") and not callable(getattr(mdl, p)):\n",
    "        print(p, getattr(mdl, p))\n",
    "        try:\n",
    "            setattr(model2, p, getattr(mdl, p))\n",
    "        except:\n",
    "            print(f\">>>>>>Error setting {p}\")\n",
    "\n",
    "\n",
    "# Load the booster model into the new classifier\n",
    "model2._Booster = booster_model\n",
    "model2.fitted_ = True\n",
    "\n",
    "# overlay the classifier in the pipeline with the reconstructed model\n",
    "# pipeline2.named_steps['lgbm'] = None #model2\n",
    "pipeline2.__dict__[\"steps\"][1] = (\"lgbm\", model2)\n",
    "\n",
    "print(pipeline2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Acc: 0.9510\n",
      "Test AUC: 0.9510\n",
      "[[0.00803537 0.99196463]\n",
      " [0.05653244 0.94346756]\n",
      " [0.04401297 0.95598703]\n",
      " [0.99335159 0.00664841]\n",
      " [0.84520952 0.15479048]\n",
      " [0.00725711 0.99274289]\n",
      " [0.00394439 0.99605561]\n",
      " [0.97750684 0.02249316]\n",
      " [0.03604028 0.96395972]\n",
      " [0.13477993 0.86522007]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = test_data.drop('target', axis=1)\n",
    "y_test = test_data['target']\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = pipeline2.predict(X_test)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_roc_auc = roc_auc_score(y_test, test_predictions)\n",
    "\n",
    "print()\n",
    "print(f\"Test Acc: {test_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# Get probability predictions\n",
    "model2_proba = pipeline2.predict_proba(X_test)\n",
    "\n",
    "print(model2_proba[:10])\n",
    "\n",
    "# Save the model as pickle file\n",
    "with open('pipeline3.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse the rebuilt LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Acc: 0.9510\n",
      "Test AUC: 0.9510\n",
      "[[0.00803537 0.99196463]\n",
      " [0.05653244 0.94346756]\n",
      " [0.04401297 0.95598703]\n",
      " [0.99335159 0.00664841]\n",
      " [0.84520952 0.15479048]\n",
      " [0.00725711 0.99274289]\n",
      " [0.00394439 0.99605561]\n",
      " [0.97750684 0.02249316]\n",
      " [0.03604028 0.96395972]\n",
      " [0.13477993 0.86522007]]\n",
      "probability match: True\n"
     ]
    }
   ],
   "source": [
    "# restore model from pickle file\n",
    "with open('pipeline3.pkl', 'rb') as f:\n",
    "    pipeline3 = pickle.load(f)\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = pipeline3.predict(X_test)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_roc_auc = roc_auc_score(y_test, test_predictions)\n",
    "\n",
    "print()\n",
    "print(f\"Test Acc: {test_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# Get probability predictions\n",
    "model3_proba = pipeline3.predict_proba(X_test)\n",
    "\n",
    "print(model3_proba[:10])\n",
    "\n",
    "\n",
    "print(f\"probability match: {np.all(model2_proba == model3_proba)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
